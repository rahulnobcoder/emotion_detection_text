{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3176,"sourceType":"datasetVersion","datasetId":1835},{"sourceId":3106022,"sourceType":"datasetVersion","datasetId":1896606},{"sourceId":8591831,"sourceType":"datasetVersion","datasetId":5139337}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:56:06.185113Z","iopub.execute_input":"2024-06-08T09:56:06.185978Z","iopub.status.idle":"2024-06-08T09:56:06.190033Z","shell.execute_reply.started":"2024-06-08T09:56:06.185944Z","shell.execute_reply":"2024-06-08T09:56:06.189002Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_data=pd.read_csv(\"/kaggle/input/text-dataset/training.csv\")\ntest_data=pd.read_csv(\"/kaggle/input/text-dataset/test.csv\")\nval_data=pd.read_csv(\"/kaggle/input/text-dataset/validation.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:56:06.201391Z","iopub.execute_input":"2024-06-08T09:56:06.201660Z","iopub.status.idle":"2024-06-08T09:56:06.244218Z","shell.execute_reply.started":"2024-06-08T09:56:06.201636Z","shell.execute_reply":"2024-06-08T09:56:06.243465Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:56:06.245932Z","iopub.execute_input":"2024-06-08T09:56:06.246703Z","iopub.status.idle":"2024-06-08T09:56:06.255454Z","shell.execute_reply.started":"2024-06-08T09:56:06.246668Z","shell.execute_reply":"2024-06-08T09:56:06.254624Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0                            i didnt feel humiliated      0\n1  i can go from feeling so hopeless to so damned...      0\n2   im grabbing a minute to post i feel greedy wrong      3\n3  i am ever feeling nostalgic about the fireplac...      2\n4                               i am feeling grouchy      3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i didnt feel humiliated</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i can go from feeling so hopeless to so damned...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>im grabbing a minute to post i feel greedy wrong</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i am ever feeling nostalgic about the fireplac...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i am feeling grouchy</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from collections import Counter \nCounter(train_data['label'])","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:56:06.256711Z","iopub.execute_input":"2024-06-08T09:56:06.257504Z","iopub.status.idle":"2024-06-08T09:56:06.270133Z","shell.execute_reply.started":"2024-06-08T09:56:06.257470Z","shell.execute_reply":"2024-06-08T09:56:06.269267Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Counter({1: 5362, 0: 4666, 3: 2159, 4: 1937, 2: 1304, 5: 572})"},"metadata":{}}]},{"cell_type":"code","source":"index = train_data[train_data.duplicated() == True].index\ntrain_data.drop(index, axis = 0, inplace = True)\nindex = train_data[train_data['text'].duplicated() == True].index\ntrain_data.drop(index, axis = 0, inplace = True)\ntrain_data.reset_index(inplace=True, drop = True)\ntrain_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatization(text):\n    doc = nlp(text)\n    return \" \".join([token.lemma_ for token in doc])\n\ndef remove_stop_words(text):\n\n    Text=[i for i in str(text).split() if i not in stop_words]\n    return \" \".join(Text)\n\ndef Removing_numbers(text):\n    text=''.join([i for i in text if not i.isdigit()])\n    return text\n\ndef lower_case(text):\n    \n    text = text.split()\n\n    text=[y.lower() for y in text]\n    \n    return \" \" .join(text)\n\ndef Removing_punctuations(text):\n    ## Remove punctuations\n    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n    text = text.replace('؛',\"\", )\n    \n    ## remove extra whitespace\n    text = re.sub('\\s+', ' ', text)\n    text =  \" \".join(text.split())\n    return text.strip()\n\ndef Removing_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ndef remove_small_sentences(df):\n    for i in range(len(df)):\n        if len(df.text.iloc[i].split()) < 3:\n            df.text.iloc[i] = np.nan\n            \ndef normalize_text(df):\n    tqdm.pandas()\n    df.text=df.text.progress_apply(lambda text : lower_case(text))\n    df.text=df.text.progress_apply(lambda text : remove_stop_words(text))\n    df.text=df.text.progress_apply(lambda text : Removing_numbers(text))\n    df.text=df.text.progress_apply(lambda text : Removing_punctuations(text))\n    df.text=df.text.progress_apply(lambda text : Removing_urls(text))\n    df.text=df.text.progress_apply(lambda text : lemmatization(text))\n    return df\n\ndef normalized_sentence(sentence):\n    sentence= lower_case(sentence)\n    sentence= remove_stop_words(sentence)\n    sentence= Removing_numbers(sentence)\n    sentence= Removing_punctuations(sentence)\n    sentence= Removing_urls(sentence)\n    sentence= lemmatization(sentence)\n    return sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport re\nfrom tqdm import tqdm\nfrom nltk.stem import SnowballStemmer\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nstop_words = set(stopwords.words(\"english\"))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data= normalize_text(train_data)\ntest_data= normalize_text(test_data)\nval_data= normalize_text(val_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_data['text']\ny_train = train_data['label']\n\nX_test = test_data['text']\ny_test = test_data['label']\n\nX_val = val_data['text']\ny_val = val_data['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\ntokenizer = Tokenizer(oov_token='UNK')\ntokenizer.fit_on_texts(pd.concat([X_train, X_test], axis=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.document_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_test = tokenizer.texts_to_sequences(X_test)\nsequences_val = tokenizer.texts_to_sequences(X_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = max([len(t) for t in train_data['text']])\nmaxlen","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pad_sequences(sequences_train, maxlen=229, truncating='pre')\nX_test = pad_sequences(sequences_test, maxlen=229, truncating='pre')\nX_val = pad_sequences(sequences_val, maxlen=229, truncating='pre')\n\nvocabSize = len(tokenizer.index_word) + 1\nprint(f\"Vocabulary size = {vocabSize}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_glove_file = '/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\nnum_tokens = vocabSize\nembedding_dim = 200 #latent factors or features  \nhits = 0\nmisses = 0\nembeddings_index = {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\nprint(\"Found %s word vectors.\" % len(embeddings_index))\n\n# Assign word vectors to our dictionary/vocabulary\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback = EarlyStopping(\n    monitor=\"val_loss\",\n    patience=4,\n    restore_best_weights=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adam = Adam(learning_rate=0.005)\n\nmodel = Sequential()\nmodel.add(Embedding(vocabSize, 200, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=False))\nmodel.add(Bidirectional(LSTM(256, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2)))\nmodel.add(Dense(6, activation='softmax'))\nmodel.build(input_shape=(None, X_train.shape[1]))\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train,\n                    y_train,\n                    validation_data=(X_val, y_val),\n                    verbose=1,\n                    batch_size=256,\n                    epochs=30,\n                    callbacks=[callback]\n                   )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"final_model.keras\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming the model has been trained and the history object is available\n# Function to plot training history\ndef plot_training_history(history):\n    # Plot training & validation accuracy values\n    plt.figure(figsize=(14, 5))\n    \n    # Accuracy plot\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='upper left')\n    \n    # Loss plot\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper left')\n    \n    # Show the plots\n    plt.show()\n\n# Call the function to plot the graphs\nplot_training_history(history)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=np.argmax(model.predict(X_test),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,classification_report\nprint(\"accuracy of model on test dataset is \",accuracy_score(pred,y_test))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(pred,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = [\n            \"He's over the moon about being accepted to the university\",\n            \"Your point on this certain matter made me outrageous, how can you say so? This is insane.\",\n            \"I can't do it, I'm not ready to lose anything, just leave me alone\",\n            \"Merlin's beard harry, you can cast the Patronus charm! I'm amazed!\",\n            \"i lost my friend\",\n            \"Dogs are sitting by the door.\"\n            ]\nemotion_dict = {\n    0: 'sadness',\n    1: 'joy',\n    2: 'love',\n    3: 'anger',\n    4: 'fear',\n    5: 'surprise'\n}\nfor sentence in sentences:\n    print(sentence)\n    sentence = normalized_sentence(sentence)\n    sentence = tokenizer.texts_to_sequences([sentence])\n    sentence = pad_sequences(sentence, maxlen=229, truncating='pre')\n    result = np.argmax(model.predict(sentence), axis=1)\n    print(f\"{result} : {emotion_dict[result[0]]}\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Serialize the tokenizer to JSON\ntokenizer_json = tokenizer.to_json()\n\n# Save the JSON to a file\nwith open('tokenizer.json', 'w') as f:\n    f.write(tokenizer_json)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install SpeechRecognition\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import speech_recognition as sr\n\n# Initialize the recognizer\nr = sr.Recognizer()\n\n# Audio file path\nAUDIO_FILE = \"/kaggle/input/indian-emotional-speech-corpora-iesc/Indian Emotional Speech Corpora (IESC)/Speaker-4/Anger/A-1-4-1.wav\"\n\n# Load audio file\nwith sr.AudioFile(AUDIO_FILE) as source:\n    # Record the audio data\n    audio_data = r.record(source)\n    \n    try:\n        print(\"Recognizing speech...\")\n        # Use Google Speech Recognition\n        text = r.recognize_google(audio_data)\n        print(\"Speech recognized:\")\n        print(text)\n    except sr.UnknownValueError:\n        print(\"Could not understand audio\")\n    except sr.RequestError as e:\n        print(\"Error accessing Google Speech Recognition service; {0}\".format(e))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences=[\"kids are talking by the door\"]\nfor sentence in sentences:\n    print(sentence)\n    sentence = normalized_sentence(sentence)\n    sentence = tokenizer.texts_to_sequences([sentence])\n    sentence = pad_sequences(sentence, maxlen=229, truncating='pre')\n    result = np.argmax(model.predict(sentence), axis=1)\n    print(f\"{result} : {emotion_dict[result[0]]}\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}